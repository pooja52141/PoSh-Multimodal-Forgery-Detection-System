{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["\"\"\"\n","# **PoSh Multimodal Forgery Detection System**\n","\n","# **Mount Drive**\n","\"\"\"\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install audio-extract\n","!pip install moviepy\n","!pip install facenet_pytorch #change made\n","!pip install timm\n","\n","%cd /content/drive/Shareddrives/FYP/Swin-Transformer\n","\n","# Imports Specific to Swin Transformer\n","from swin_functions_and_classes import *\n","from going_modular.going_modular import engine\n","from helper_functions import set_seeds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y0GUpAdFo74k","outputId":"5f2e4675-893b-423c-bd3f-2cf9b8e8a7f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting audio-extract\n","  Downloading audio_extract-0.6.0-py3-none-any.whl (7.5 kB)\n","Collecting ffmpeg-python==0.2.0 (from audio-extract)\n","  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n","Collecting imageio-ffmpeg==0.4.8 (from audio-extract)\n","  Downloading imageio_ffmpeg-0.4.8-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mutagen==1.46.0 (from audio-extract)\n","  Downloading mutagen-1.46.0-py3-none-any.whl (193 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python==0.2.0->audio-extract) (0.18.3)\n","Installing collected packages: mutagen, imageio-ffmpeg, ffmpeg-python, audio-extract\n","  Attempting uninstall: imageio-ffmpeg\n","    Found existing installation: imageio-ffmpeg 0.4.9\n","    Uninstalling imageio-ffmpeg-0.4.9:\n","      Successfully uninstalled imageio-ffmpeg-0.4.9\n","Successfully installed audio-extract-0.6.0 ffmpeg-python-0.2.0 imageio-ffmpeg-0.4.8 mutagen-1.46.0\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.2)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.25.2)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n","Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.8)\n","Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n","Collecting facenet_pytorch\n","  Downloading facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (1.25.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (2.31.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (0.17.1+cu121)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from facenet_pytorch) (9.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->facenet_pytorch) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->facenet_pytorch) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->facenet_pytorch) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->facenet_pytorch) (2024.2.2)\n","Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchvision->facenet_pytorch) (2.2.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision->facenet_pytorch) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision->facenet_pytorch) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision->facenet_pytorch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision->facenet_pytorch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision->facenet_pytorch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision->facenet_pytorch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1->torchvision->facenet_pytorch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1->torchvision->facenet_pytorch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1->torchvision->facenet_pytorch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1->torchvision->facenet_pytorch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1->torchvision->facenet_pytorch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1->torchvision->facenet_pytorch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1->torchvision->facenet_pytorch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1->torchvision->facenet_pytorch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1->torchvision->facenet_pytorch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1->torchvision->facenet_pytorch)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1->torchvision->facenet_pytorch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision->facenet_pytorch) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchvision->facenet_pytorch)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchvision->facenet_pytorch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchvision->facenet_pytorch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, facenet_pytorch\n","Successfully installed facenet_pytorch-2.5.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n","Collecting timm\n","  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.2.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.13.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.11.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.3)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.4.127)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n","Installing collected packages: timm\n","Successfully installed timm-0.9.16\n","/content/drive/Shareddrives/FYP/Swin-Transformer\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQkSgGXgoShJ"},"outputs":[],"source":["def multimodal_forgery_detection(Video):\n","\n","    \"\"\"# **Imports**\"\"\"\n","\n","    import os\n","    import shutil\n","    from google.colab import files\n","    from IPython.display import HTML, clear_output\n","    from base64 import b64encode\n","    import moviepy.editor as mp\n","\n","    from audio_extract import extract_audio\n","    from moviepy.editor import VideoFileClip\n","    import librosa.display\n","    import numpy as np\n","    import matplotlib.pyplot as plt\n","    from google.colab.patches import cv2_imshow\n","\n","    import PIL\n","    from PIL import Image\n","    import torch\n","    import torch.nn as nn\n","    from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","    import torchvision.transforms as transforms\n","    from torchvision import datasets, transforms\n","    from torch.utils.data import DataLoader, TensorDataset\n","\n","    import cv2\n","\n","    import warnings\n","    import pandas as pd\n","    import tensorflow as tf\n","    from tensorflow import keras\n","    from tensorflow.keras import layers\n","\n","    import torchvision.models as models\n","    import dlib\n","\n","    from IPython.display import HTML, Audio, clear_output\n","    from google.colab.output import eval_js\n","    from base64 import b64decode\n","    from scipy.io.wavfile import read as wav_read\n","    import io\n","    import ffmpeg\n","\n","    \"\"\"**Upload Video**\"\"\"\n","\n","    if not os.path.isfile(Video):\n","        print(\"ERROR: File not found!\")\n","        raise SystemExit(0)\n","    else:\n","        shutil.copy(Video, '/content/input_video.mp4')\n","\n","    \"\"\"### **Split input video into audio, video and extract mel spectrogram**\n","\n","    **Extract Mel Spectrogram**\n","    \"\"\"\n","\n","    def extract_mel_spectrogram(output_audio_path, output_image_path):\n","        y, sr = librosa.load(output_audio_path)\n","        # Compute the Mel spectrogram\n","        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=sr//2)\n","        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n","        # Plot the Mel spectrogram\n","        plt.figure(figsize=(10, 4))\n","        librosa.display.specshow(mel_spec_db, x_axis='time', y_axis='mel', sr=sr, fmax=sr//2)\n","        plt.colorbar(format='%+2.0f dB')\n","        plt.title('Mel spectrogram')\n","        plt.tight_layout()\n","        # Save the image\n","        plt.savefig(output_image_path)\n","\n","    \"\"\"**Save Audio and Melspectrogram**\"\"\"\n","\n","    # Define the video file path\n","    video_path = \"/content/input_video.mp4\"\n","\n","    # Load the video clip\n","    video_clip = VideoFileClip(video_path)\n","\n","    # Extract the audio from the video\n","    audio_clip = video_clip.audio\n","\n","    # Define the audio file path\n","    audio_path = \"/content/audio.mp3\"\n","\n","    # Write the extracted audio to a file\n","    audio_clip.write_audiofile(audio_path, codec='mp3')  # You can use other audio codecs like 'wav' if needed\n","\n","    # Close the audio and video clips\n","    audio_clip.close()\n","    video_clip.close()\n","\n","    # Define the melspectrogram path\n","    melspectrogram_path = \"/content/melspectrogram.png\"\n","    extract_mel_spectrogram(audio_path, melspectrogram_path)\n","\n","    print(\"Video saved to:\", video_path)\n","    print(\"Audio Saved to:\", audio_path)\n","    print(\"Melspectrogram Saved to:\", melspectrogram_path)\n","\n","    \"\"\"# **Audio Network**\"\"\"\n","\n","    \"\"\"**Prediction**\"\"\"\n","\n","    # import torchvision.transforms as transforms\n","\n","    # Create image size\n","    IMG_SIZE = 224\n","\n","    # Create transform pipeline manually\n","    manual_transforms = transforms.Compose([\n","        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","        transforms.ToTensor(),\n","    ])\n","\n","    # Define the class names\n","    class_names = ['fake', 'real']\n","\n","    model = SwinTransformer(num_classes=len(class_names))\n","\n","    # Load saved model\n","    model.load_state_dict(torch.load('/content/drive/Shareddrives/FYP/trained_model_SWIN_8_1e-4_0.01_5epochs.pth'))\n","    model.eval()\n","\n","    # Open melspectrogram\n","    image = Image.open('/content/melspectrogram.png')\n","    image_rgb = image.convert(\"RGB\")\n","\n","    # Apply the transformation to convert the image to a tensor\n","    image_tensor = manual_transforms(image_rgb).unsqueeze(0)  # Add batch dimension\n","\n","    # Perform inference\n","    with torch.no_grad():\n","        outputs = model(image_tensor)\n","        _, predicted = torch.max(outputs, 1)\n","        audio_network_prediction = class_names[predicted[0]]\n","\n","    \"\"\"#**Video Network**\"\"\"\n","\n","    %run /content/drive/Shareddrives/FYP2/Video_Swin_Transformer/predict_folder.py --weights-dir /content/drive/Shareddrives/FYP2/Video_Swin_Transformer/weights --models video_swin_weights --test-dir /content/\n","    %store video_network_prediction\n","\n","    \"\"\"# **Audio-Visual Network**\n","\n","    ### **Wav2lip**\n","\n","    **Setup**\n","    \"\"\"\n","\n","    %cd /content/\n","\n","    !rm -rf /content/sample_data\n","    !mkdir /content/sample_data\n","\n","    !git clone https://github.com/justinjohn0306/Wav2Lip\n","\n","    %cd /content/Wav2Lip\n","\n","    #download the pretrained model\n","    !wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/wav2lip.pth' -O 'checkpoints/wav2lip.pth'\n","    !wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/wav2lip_gan.pth' -O 'checkpoints/wav2lip_gan.pth'\n","    !wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/resnet50.pth' -O 'checkpoints/resnet50.pth'\n","    !wget 'https://github.com/justinjohn0306/Wav2Lip/releases/download/models/mobilenet.pth' -O 'checkpoints/mobilenet.pth'\n","    !pip install git+https://github.com/elliottzheng/batch-face.git@master\n","\n","    !pip install ffmpeg-python mediapipe==0.8.11\n","\n","    %cd content\n","\n","    def showVideo(path):\n","      mp4 = open(str(path),'rb').read()\n","      data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","      return HTML(\"\"\"\n","      <video width=700 controls>\n","            <source src=\"%s\" type=\"video/mp4\">\n","      </video>\n","      \"\"\" % data_url)\n","\n","    clear_output()\n","\n","    \"\"\"**Lip Sync**\"\"\"\n","\n","    %cd /content/Wav2Lip\n","\n","    # Set up paths and variables for the output file\n","    output_file_path = '/content/Wav2Lip/results/result_voice.mp4'\n","\n","    # Delete existing output file before processing, if any\n","    if os.path.exists(output_file_path):\n","        os.remove(output_file_path)\n","\n","    pad_top =  0\n","    pad_bottom =  10\n","    pad_left =  0\n","    pad_right =  0\n","    rescaleFactor =  1\n","    nosmooth = True\n","    use_hd_model = True\n","\n","    checkpoint_path = 'checkpoints/wav2lip.pth' if not use_hd_model else 'checkpoints/wav2lip_gan.pth'\n","\n","\n","    if nosmooth == False:\n","      !python inference.py --checkpoint_path $checkpoint_path --face \"/content/input_video.mp4\" --audio \"/content/audio.mp3\" --pads $pad_top $pad_bottom $pad_left $pad_right --resize_factor $rescaleFactor\n","    else:\n","      !python inference.py --checkpoint_path $checkpoint_path --face \"/content/input_video.mp4\" --audio \"/content/audio.mp3\" --pads $pad_top $pad_bottom $pad_left $pad_right --resize_factor $rescaleFactor --nosmooth\n","\n","    #Preview output video\n","    if os.path.exists(output_file_path):\n","        clear_output()\n","        print(\"Final Video Preview\")\n","        print(\"Download this video from\", output_file_path)\n","        showVideo(output_file_path)\n","    else:\n","        print(\"Processing failed. Output video not found.\")\n","\n","    shutil.copy('/content/Wav2Lip/results/result_voice.mp4', '/content/wav2lip.mp4')\n","\n","    \"\"\"### **Real-ESRGAN**\n","\n","    **Setup**\n","    \"\"\"\n","\n","    # Clone Real-ESRGAN and enter the Real-ESRGAN\n","    %cd /content/\n","    !git clone https://github.com/xinntao/Real-ESRGAN.git\n","    %cd /content/Real-ESRGAN\n","    # Set up the environment\n","    !pip install basicsr\n","    !pip install facexlib\n","    !pip install gfpgan\n","    !pip install ffmpeg-python\n","    !pip install -r requirements.txt\n","    !python setup.py develop\n","\n","    \"\"\"**Fix basicsr Error**\"\"\"\n","\n","    def read_file_to_list(input_file_path):\n","        with open(input_file_path, 'r') as file:\n","            lines = file.readlines()\n","        return lines\n","\n","    def write_list_to_file(lines, output_file_path):\n","        with open(output_file_path, 'w') as file:\n","            file.writelines(lines)\n","\n","    # Example usage\n","    input_file_path = '/usr/local/lib/python3.10/dist-packages/basicsr/data/degradations.py'  # Replace with your input file path\n","    output_file_path = '/usr/local/lib/python3.10/dist-packages/basicsr/data/degradations.py'  # Replace with your output file path\n","\n","    # Read contents of input file into a list\n","    lines = read_file_to_list(input_file_path)\n","    lines[7] = \"from torchvision.transforms.functional import rgb_to_grayscale\\n\"\n","\n","    # Write the list into another file\n","    write_list_to_file(lines, output_file_path)\n","\n","    \"\"\"**Create Folders for Videos**\"\"\"\n","\n","    upload_folder = '/content/Real-ESRGAN/upload'\n","    result_folder = '/content/Real-ESRGAN/results'\n","\n","    if os.path.isdir(upload_folder):\n","        shutil.rmtree(upload_folder)\n","    if os.path.isdir(result_folder):\n","        shutil.rmtree(result_folder)\n","    os.mkdir(upload_folder)\n","    os.mkdir(result_folder)\n","\n","    shutil.copy('/content/input_video.mp4', upload_folder + \"/input_video.mp4\")\n","    shutil.copy('/content/wav2lip.mp4', upload_folder + \"/wav2lip.mp4\")\n","\n","    \"\"\"**Improve Resolution**\"\"\"\n","\n","    ! python inference_realesrgan_video.py -i upload/input_video.mp4 -n realesr-animevideov3 -s 2 #--suffix out\n","    # Arguments\n","    # -i, --input: input video\n","    # -n, --model_name: Used model name\n","    # -s, --outscale: Scale\n","    # -suffix: Suffix of the output video\n","\n","    ! python inference_realesrgan_video.py -i upload/wav2lip.mp4 -n realesr-animevideov3 -s 2\n","\n","    \"\"\"**Move Videos to '/content/'**\"\"\"\n","\n","    shutil.copy(result_folder + '/input_video_out.mp4', \"/content/input_video_out.mp4\")\n","    shutil.copy(result_folder + '/wav2lip_out.mp4', \"/content/wav2lip_out.mp4\")\n","\n","    \"\"\"### **ResNet18**\n","\n","    **Extract Embeddings**\n","    \"\"\"\n","\n","    import torch\n","    import torchvision.transforms as transforms\n","    import torchvision.models as models\n","    import cv2\n","    import dlib\n","    from PIL import Image\n","    # Load the pre-trained face detector from dlib\n","    detector = dlib.get_frontal_face_detector()\n","\n","    # Preprocess the video frames\n","    def preprocess_frame(frame):\n","        transform = transforms.Compose([\n","            transforms.Resize((224, 224)),  # Resize to match ResNet input size\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n","        ])\n","        return transform(frame)\n","\n","    # Load ResNet model\n","    resnet = models.resnet18(pretrained=True)\n","    resnet.fc = nn.Identity()  # Remove the fully connected layer\n","\n","    # Extract features from video\n","    def extract_features(video_path):\n","        video_capture = cv2.VideoCapture(video_path)\n","        features = []\n","        while True:\n","            ret, frame = video_capture.read()\n","            if not ret:\n","                break\n","\n","            # Detect faces in the frame\n","            faces = detector(frame)\n","\n","            for face in faces:\n","                # Get the coordinates of the face\n","                x, y, w, h = face.left(), face.top(), face.width(), face.height()\n","\n","                # Define region of interest (ROI) for lips detection\n","                lips_roi = frame[y + h//2 : y + h, x : x + w]\n","                # cv2_imshow(lips_roi)\n","\n","            frame = Image.fromarray(cv2.cvtColor(lips_roi, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB\n","            preprocessed_frame = preprocess_frame(frame)\n","\n","\n","            with torch.no_grad():\n","                features.append(resnet(preprocessed_frame.unsqueeze(0)))\n","        video_capture.release()\n","        return torch.stack(features)\n","\n","    # Aggregate features\n","    def aggregate_features(features):\n","        # Simple aggregation by averaging the features\n","        return torch.mean(features, dim=0)\n","\n","\n","    video_path = '/content/input_video_out.mp4'\n","    video_features = extract_features(video_path)\n","    video_embedding = aggregate_features(video_features)\n","    # Expected output: torch.Size([512])\n","\n","    syn_video_path = '/content/wav2lip_out.mp4'\n","    syn_video_features = extract_features(syn_video_path)\n","    syn_video_embedding = aggregate_features(syn_video_features)\n","    # Expected output: torch.Size([512])\n","\n","    \"\"\"### **MS-TCN**\n","\n","    **Setup**\n","    \"\"\"\n","\n","    %cd /content/\n","    !git clone https://github.com/ahaliassos/LipForensics.git\n","    %cd /content/LipForensics/models\n","\n","    from tcn import MultibranchTemporalConvNet\n","\n","    \"\"\"**Testing**\"\"\"\n","\n","    import torch\n","    from torch.utils.data import DataLoader, TensorDataset\n","    from tcn import MultibranchTemporalConvNet\n","\n","    # Assuming test_embeddings is a list of tensors\n","    test_embeddings = [video_embedding - syn_video_embedding]\n","\n","    # Convert data to PyTorch tensors\n","    test_embeddings_tensor = torch.stack(test_embeddings)\n","    test_embeddings_tensor = test_embeddings_tensor.permute(0, 2, 1)\n","\n","    # Create TensorDataset\n","    test_dataset = TensorDataset(test_embeddings_tensor)\n","\n","    # Create DataLoader\n","    batch_size = 32\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Load the pre-trained model\n","    model = MultibranchTemporalConvNet(num_inputs=512, num_channels=[60, 120, 240, 480], tcn_options={\"kernel_size\": [3, 5, 7]})\n","    model.load_state_dict(torch.load(\"/content/drive/Shareddrives/FYP/tcn_model_new_ss_196.pth\"))\n","    model.eval()\n","\n","    # Apply threshold for classification\n","    threshold = 0.5  # Adjust this threshold based on your needs\n","\n","    # Testing loop\n","    predicted_labels = []\n","    with torch.no_grad():\n","        for data in test_loader:\n","            data = data[0]  # Extract the tensor from the list\n","            outputs = model(data)\n","            probabilities = torch.sigmoid(outputs)  # Apply sigmoid activation to convert logits to probabilities\n","            labels = (probabilities > threshold).float()  # Apply threshold for classification\n","            predicted_labels.append(labels)\n","\n","    # Concatenate predicted labels\n","    predicted_labels = torch.cat(predicted_labels, dim=0)\n","\n","    num_fake_frames = torch.sum(predicted_labels > threshold).item()\n","    total_frames = predicted_labels[0].size(0)\n","    fake_percentage = num_fake_frames / total_frames\n","\n","    if fake_percentage > 0.5:\n","        audio_visual_network_prediction = 'fake'\n","    else:\n","        audio_visual_network_prediction = 'real'\n","\n","    \"\"\"# **OR Module**\"\"\"\n","\n","    clear_output()\n","\n","    forgery = \"\"\n","    if audio_network_prediction == 'fake' or video_network_prediction == 'fake' or audio_visual_network_prediction == 'fake':\n","        forgery += \"Forged\"\n","    else:\n","        forgery += \"Not Forged\"\n","\n","    return (audio_network_prediction,\n","            video_network_prediction,\n","            audio_visual_network_prediction,\n","            forgery,\n","            '/content/melspectrogram.png',\n","            '/content/input_video_out.mp4',\n","            '/content/wav2lip_out.mp4')"]},{"cell_type":"code","source":["!pip install gradio"],"metadata":{"id":"nqBMFB5vOTdF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc59470e-a6ae-43fd-e1f8-369229636e81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio\n","  Downloading gradio-4.27.0-py3-none-any.whl (17.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n","Collecting fastapi (from gradio)\n","  Downloading fastapi-0.110.2-py3-none-any.whl (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ffmpy (from gradio)\n","  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client==0.15.1 (from gradio)\n","  Downloading gradio_client-0.15.1-py3-none-any.whl (313 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n","Collecting orjson~=3.0 (from gradio)\n","  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.0)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting python-multipart>=0.0.9 (from gradio)\n","  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n","Collecting ruff>=0.2.2 (from gradio)\n","  Downloading ruff-0.4.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Collecting typer<1.0,>=0.12 (from gradio)\n","  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.11.0)\n","Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n","Collecting uvicorn>=0.14.0 (from gradio)\n","  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.15.1->gradio) (2023.6.0)\n","Collecting websockets<12.0,>=10.0 (from gradio-client==0.15.1->gradio)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n","Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.1)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n","  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n","Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n","  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.34.0)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Building wheels for collected packages: ffmpy\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=94078000b425e756ec5d8d8f7456aaa4a406c1d8e2b230d76d180a4b5d041281\n","  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n","Successfully built ffmpy\n","Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, h11, aiofiles, uvicorn, starlette, httpcore, typer, httpx, fastapi, gradio-client, gradio\n","  Attempting uninstall: typer\n","    Found existing installation: typer 0.9.4\n","    Uninstalling typer-0.9.4:\n","      Successfully uninstalled typer-0.9.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n","weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiofiles-23.2.1 fastapi-0.110.2 ffmpy-0.3.2 gradio-4.27.0 gradio-client-0.15.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 orjson-3.10.1 pydub-0.25.1 python-multipart-0.0.9 ruff-0.4.1 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 typer-0.12.3 uvicorn-0.29.0 websockets-11.0.3\n"]}]},{"cell_type":"code","source":["import gradio as gr\n","\n","iface = gr.Interface(\n","    fn=multimodal_forgery_detection,\n","    inputs=[gr.Video(label=\"Input Video\")],\n","    outputs=[gr.Textbox(label=\"Audio Network Prediction\"),\n","             gr.Textbox(label=\"Video Network Prediction\"),\n","             gr.Textbox(label=\"Audio-Visual Network Prediction\"),\n","             gr.Textbox(label=\"Multimodal Forgery Detection\"),\n","             gr.Image(label=\"Mel Spectrogram\"),\n","             gr.Video(label=\"Input Video with Improved Resolution\"),\n","             gr.Video(label=\"Wav2lip-Generated Video with Improved Resolution\")],\n","    title=\"PoSh Multimodal Forgery Detection System\",\n","    description=\"\",\n","    allow_flagging=False,\n","    examples=[[\"/content/drive/Shareddrives/FYP/dataset/video/fake_video-fake_audio/Asian (East)/men/fvfa_Asian_East_men_0.mp4\"],\n","              [\"/content/drive/Shareddrives/FYP/dataset/video/real_video-real_audio/Caucasian (American)/women/rvra_Caucasian_American_women_14.mp4\"],\n","              [\"/content/drive/Shareddrives/FYP/testcases/testcase.mp4\"],\n","              [\"/content/drive/Shareddrives/FYP/testcases/testcase1.mp4\"],\n","              [\"/content/drive/Shareddrives/FYP/testcases/testcase2.mp4\"],\n","              [\"/content/drive/Shareddrives/FYP/mstcn_videos/fvra_Caucasian_European_men_0.mp4\"],\n","              [\"/content/drive/Shareddrives/FYP/dataset/video/real_video-fake_audio/African/women/rvfa_African_women_43.mp4\"],\n","              [\"/content/drive/Shareddrives/FYP/dataset/mstcn/fake/nonsync.mp4\"]],\n","    theme='gradio/soft'\n",")\n","iface.launch()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686},"id":"1N4FAT71Qyyj","outputId":"dde25170-2516-41b3-ab60-2f7500390432"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gradio/interface.py:374: UserWarning: The `allow_flagging` parameter in `Interface` nowtakes a string value ('auto', 'manual', or 'never'), not a boolean. Setting parameter to: 'never'.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Running on public URL: https://7c2e495ef3137ebe97.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://7c2e495ef3137ebe97.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":5}]}]}